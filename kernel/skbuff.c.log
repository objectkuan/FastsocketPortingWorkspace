diff --git a/kernel/net/core/skbuff.c b/kernel/net/core/skbuff.c
index dc4a95c..278258c 100644
--- a/kernel/net/core/skbuff.c
+++ b/kernel/net/core/skbuff.c
@@ -70,8 +70,14 @@
 
 #include "kmap_skb.h"
 
+#define DPRINTK(msg, args...) 
+
+//#define DPRINTK(msg, args...) printk(KERN_DEBUG "Fastsocket [CPU%d][PID-%d] %s:%d\t" msg, smp_processor_id(), current->pid, __FUNCTION__, __LINE__, ## args);
+
 static struct kmem_cache *skbuff_head_cache __read_mostly;
 static struct kmem_cache *skbuff_fclone_cache __read_mostly;
+static struct kmem_cache *fastsocket_skbuff_head_cache __read_mostly;
+static struct kmem_cache *fastsocket_skbuff_fclone_cache __read_mostly;
 
 static void sock_pipe_buf_release(struct pipe_inode_info *pipe,
 				  struct pipe_buffer *buf)
@@ -169,26 +175,106 @@ EXPORT_SYMBOL(skb_under_panic);
  *	Buffers may only be allocated from interrupts using a @gfp_mask of
  *	%GFP_ATOMIC.
  */
+
+int enable_skb_pool = 0;
+struct skb_pool __percpu *skb_pools;
+
+EXPORT_SYMBOL(enable_skb_pool);
+
 struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 			    int fclone, int node)
 {
-	struct kmem_cache *cache;
+	struct kmem_cache *cache = NULL;
+	struct skb_pool *pool = NULL;
+	struct sk_buff_head *free_list = NULL, *recyc_list = NULL;
 	struct skb_shared_info *shinfo;
 	struct sk_buff *skb;
+	int clone;
 	u8 *data;
 
-	cache = fclone ? skbuff_fclone_cache : skbuff_head_cache;
+	pool = per_cpu_ptr(skb_pools, smp_processor_id());
+
+	size = SKB_DATA_ALIGN(size);
+	//cache = fclone ? skbuff_fclone_cache : skbuff_head_cache;
+
+	switch (fclone) {
+		case SLAB_SKB:
+			cache = skbuff_head_cache;
+			clone = 0;
+			pool->slab_hit++;
+			break;
+		case SLAB_SKB_CLONE:
+			cache = skbuff_fclone_cache;
+			clone = 1;
+			pool->clone_slab_hit++;
+			break;
+		case POOL_SKB:
+			free_list = &pool->free_list;
+			recyc_list = &pool->recyc_list;
+			cache = skbuff_head_cache;
+			clone = 0;
+			pool->pool_hit++;
+			break;
+		case POOL_SKB_CLONE:
+			free_list = &pool->clone_free_list;
+			recyc_list = &pool->clone_recyc_list;
+			cache = skbuff_fclone_cache;
+			clone = 1;
+			pool->clone_pool_hit++;
+			break;
+		default:
+			return NULL;
+	}
+
+	DPRINTK("Enable skb_pool %d, skb clone %d-%d, slab %s, free_list 0x%p, recyc_list 0x%p, packet size %d\n", 
+			enable_skb_pool, clone, fclone, cache->name, free_list, recyc_list, size);
+
+	if (enable_skb_pool && free_list && recyc_list && 
+			size < MAX_FASTSOCKET_SKB_DATA_SIZE) {
+		//unsigned long flags;
+
+		//local_irq_save(flags);
+		//local_bh_disable();
+		skb = __skb_dequeue(free_list);
+		//local_bh_enable();
+		//local_irq_restore(flags);
+		//if (likely(skb))
+		//	DPRINTK("Allocate skb[%d] 0x%p from %d free list\n", 
+		//			skb->pool_id, skb, smp_processor_id());
+		if (unlikely(!skb)) {
+			unsigned long flags;
+
+			DPRINTK("Splice %u skbs from recycle list\n", recyc_list->qlen);
+
+			spin_lock_irqsave(&recyc_list->lock, flags);
+			skb_queue_splice_init(recyc_list, free_list);
+			spin_unlock_irqrestore(&recyc_list->lock, flags);
+
+			skb = skb_dequeue(free_list);
+		}
+		if (skb) {
+			data = skb->data_cache;
+			skb->pool_id = smp_processor_id();
+			goto init;
+		}
+		//} else 
+		//	DPRINTK("Allocate skb failed from %d pool list\n", 
+		//			smp_processor_id());
+	}
 
 	/* Get the HEAD */
 	skb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);
 	if (!skb)
 		goto out;
+	DPRINTK("Allocate regular skb[%d] 0x%p\n", skb->pool_id, skb);
 
-	size = SKB_DATA_ALIGN(size);
+	//size = SKB_DATA_ALIGN(size);
 	data = kmalloc_node_track_caller(size + sizeof(struct skb_shared_info),
 			gfp_mask, node);
 	if (!data)
 		goto nodata;
+init:
+	DPRINTK("Initialize skb[%d] %p\n", skb->pool_id, skb);
 
 	/*
 	 * Only clear those fields we need to clear, not those that we will
@@ -220,7 +306,7 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 	skb_frag_list_init(skb);
 	memset(&shinfo->hwtstamps, 0, sizeof(shinfo->hwtstamps));
 
-	if (fclone) {
+	if (clone) {
 		struct sk_buff *child = skb + 1;
 		atomic_t *fclone_ref = (atomic_t *) (child + 1);
 
@@ -259,7 +345,13 @@ struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
 	int node = dev->dev.parent ? dev_to_node(dev->dev.parent) : -1;
 	struct sk_buff *skb;
 
-	skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, 0, node);
+	if (enable_skb_pool) {
+		skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, POOL_SKB, node);
+		DPRINTK("Allocate pool skb 0x%p\n", skb);
+	} else {
+		skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, SLAB_SKB, node);
+		DPRINTK("Allocate regular skb 0x%p\n", skb);
+	}
 	if (likely(skb)) {
 		skb_reserve(skb, NET_SKB_PAD);
 		skb->dev = dev;
@@ -362,10 +454,65 @@ static void skb_release_data(struct sk_buff *skb)
 		if (skb_has_frags(skb))
 			skb_drop_fraglist(skb);
 
+		if (skb->pool_id >= 0)
+			return;
+
 		kfree(skb->head);
 	}
 }
 
+static inline void kfree_pool_skb_clone(struct sk_buff *skb)
+{
+	struct skb_pool *skb_pool;
+
+	BUG_ON(skb->pool_id < 0);
+	
+	skb_pool = per_cpu_ptr(skb_pools, skb->pool_id);
+
+	if (skb->pool_id == smp_processor_id()) {
+		__skb_queue_head(&skb_pool->clone_free_list, skb);
+		//if (likely(in_softirq())) {
+		//	__skb_queue_head(&skb_pool->clone_free_list, skb);
+		//	//printk(KERN_DEBUG "Free clone pool skb 0x%p in softirq\n", skb);
+		//} else {
+		//	//printk(KERN_DEBUG "Free clone pool skb 0x%p NOT in sofrirq\n", skb);
+		//	local_bh_disable();
+		//	__skb_queue_head(&skb_pool->clone_free_list, skb);
+		//	local_bh_disable();
+		//}
+		DPRINTK("Free clone pool skb[%d] 0x%p on CPU %d into free list\n", skb->pool_id, skb, smp_processor_id());
+	} else {
+		skb_queue_head(&skb_pool->clone_recyc_list, skb);
+		DPRINTK("Free clone pool skb[%d] 0x%p on CPU %d into recycle list\n", skb->pool_id, skb, smp_processor_id());
+	}
+}
+
+static inline void kfree_pool_skb(struct sk_buff *skb)
+{
+	struct skb_pool *skb_pool;
+
+	BUG_ON(skb->pool_id < 0);
+	
+	skb_pool = per_cpu_ptr(skb_pools, skb->pool_id);
+
+	if (skb->pool_id == smp_processor_id()) {
+		if (in_softirq()) {
+			DPRINTK("Free skb[%d] 0x%p in softirq on CPU %d\n", skb->pool_id, skb, smp_processor_id());
+			//printk(KERN_DEBUG "Free pool skb 0x%p in softirq\n", skb);
+			__skb_queue_head(&skb_pool->free_list, skb);
+		} else {
+			local_bh_disable();
+			__skb_queue_head(&skb_pool->free_list, skb);
+			local_bh_enable();
+			//printk(KERN_DEBUG "Free pool skb 0x%p NOT in softirq\n", skb);
+		}
+		DPRINTK("Free clone pool skb[%d] 0x%p on CPU %d into free list\n", skb->pool_id, skb, smp_processor_id());
+	} else {
+		skb_queue_head(&skb_pool->recyc_list, skb);
+		DPRINTK("Free clone pool skb[%d] 0x%p on CPU %d into recycle list\n", skb->pool_id, skb, smp_processor_id());
+	}
+}
+
 /*
  *	Free an skbuff by memory without cleaning the state.
  */
@@ -374,15 +521,53 @@ static void kfree_skbmem(struct sk_buff *skb)
 	struct sk_buff *other;
 	atomic_t *fclone_ref;
 
+	//barrier();
+
 	switch (skb->fclone) {
 	case SKB_FCLONE_UNAVAILABLE:
-		kmem_cache_free(skbuff_head_cache, skb);
+		if (skb->pool_id >= 0) {
+			kfree_pool_skb(skb);
+			//struct skb_pool *skb_pool;
+			//
+		       	//skb_pool = per_cpu_ptr(skb_pools, skb->pool_id);
+
+			//if (skb->pool_id == smp_processor_id()) {
+			//	//unsigned long flags;
+
+			//	//local_irq_save(flags);
+			//	if (in_softirq()) {
+			//		DPRINTK("Free skb[%d] 0x%p in softirq on CPU %d\n", skb->pool_id, skb, smp_processor_id());
+			//		__skb_queue_head(&skb_pool->free_list, skb);
+			//	} else {
+			//		DPRINTK("Free skb[%d] 0x%p NOT in softirq on CPU %d\n", skb->pool_id, skb, smp_processor_id());
+			//		local_bh_disable();
+			//		__skb_queue_head(&skb_pool->free_list, skb);
+			//		_local_bh_enable();
+			//	}
+			//	//local_irq_restore(flags);
+
+			//	DPRINTK("Put skb[%d] 0x%p into %d free list\n", 
+			//			skb->pool_id, skb, skb->pool_id);
+			//}
+			//else {
+			//	skb_queue_head(&skb_pool->recyc_list, skb);
+			//	DPRINTK("Put skb[%d] 0x%p into %d recycle list\n", 
+			//			skb->pool_id, skb, skb->pool_id);
+			//}
+		} else {
+			DPRINTK("Free regular skb[%d] 0x%p\n", skb->pool_id, skb);
+			kmem_cache_free(skbuff_head_cache, skb);
+		}
 		break;
 
 	case SKB_FCLONE_ORIG:
 		fclone_ref = (atomic_t *) (skb + 2);
-		if (atomic_dec_and_test(fclone_ref))
-			kmem_cache_free(skbuff_fclone_cache, skb);
+		if (atomic_dec_and_test(fclone_ref)) {
+			if (skb->pool_id >= 0)
+				kfree_pool_skb_clone(skb);
+			else
+				kmem_cache_free(skbuff_fclone_cache, skb);
+		}
 		break;
 
 	case SKB_FCLONE_CLONE:
@@ -394,8 +579,12 @@ static void kfree_skbmem(struct sk_buff *skb)
 		 */
 		skb->fclone = SKB_FCLONE_UNAVAILABLE;
 
-		if (atomic_dec_and_test(fclone_ref))
-			kmem_cache_free(skbuff_fclone_cache, other);
+		if (atomic_dec_and_test(fclone_ref)) {
+			if (skb->pool_id >= 0)
+				kfree_pool_skb_clone(other);
+			else
+				kmem_cache_free(skbuff_fclone_cache, other);
+		}
 		break;
 	}
 }
@@ -460,6 +649,8 @@ void kfree_skb(struct sk_buff *skb)
 {
 	if (unlikely(!skb))
 		return;
+	DPRINTK("Try to free skb[%d] 0x%p[%d]\n", skb->pool_id, skb, atomic_read(&skb->users));
+
 	if (likely(atomic_read(&skb->users) == 1))
 		smp_rmb();
 	else if (likely(!atomic_dec_and_test(&skb->users)))
@@ -2866,19 +3057,178 @@ done:
 }
 EXPORT_SYMBOL_GPL(skb_gro_receive);
 
+static volatile unsigned cpu_id;
+
+static struct skb_pool *skb_pool_get_online(loff_t *pos)
+{
+	struct skb_pool *rc = NULL;
+
+	while (*pos < nr_cpu_ids)
+		if (cpu_online(*pos)) {
+			rc = per_cpu_ptr(skb_pools, *pos);
+			break;
+		} else
+			++*pos;
+	cpu_id = *pos;
+
+	return rc;
+}
+
+static void *skb_pool_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	++*pos;
+	return skb_pool_get_online(pos);
+}
+
+static void skb_pool_seq_stop(struct seq_file *seq, void *v)
+{
+
+}
+
+static void *skb_pool_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	seq_printf(seq, "%s\t%-15s%-15s%-15s%-15s%-15s%-15s%-15s%-15s\n",
+		"CPU", "Free", "Recycle", 
+		"Pool_hit", "Slab_hit", 
+		"C_free", "C_recycle", 
+		"C_pool_hit", "C_slab_hit");
+		
+	cpu_id = 0;
+
+	return skb_pool_get_online(pos);
+}
+
+static int skb_pool_seq_show(struct seq_file *seq, void *v)
+{
+	struct skb_pool *s = v;
+
+	seq_printf(seq, "%u\t%-15u%-15u%-15lu%-15lu%-15u%-15u%-15lu%-15lu\n", 
+		cpu_id, s->free_list.qlen, s->recyc_list.qlen, 
+		s->pool_hit, s->slab_hit,
+		s->clone_free_list.qlen, s->clone_recyc_list.qlen, 
+		s->clone_pool_hit, s->clone_slab_hit);
+
+	return 0;
+}
+static const struct seq_operations skb_pool_seq_ops = {
+	.start = skb_pool_seq_start,
+	.next  = skb_pool_seq_next,
+	.stop  = skb_pool_seq_stop,
+	.show  = skb_pool_seq_show,
+};
+
+static int skb_pool_seq_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &skb_pool_seq_ops);
+}
+
+ssize_t skb_pool_reset(struct file *file, const char __user *buf, size_t size, loff_t *ppos)
+{
+	int cpu;
+	struct skb_pool *skb_pool;
+
+	for_each_online_cpu(cpu) {
+		skb_pool = per_cpu_ptr(skb_pools, cpu);
+		skb_pool->slab_hit = 0;
+		skb_pool->clone_slab_hit = 0;
+		skb_pool->pool_hit = 0;
+		skb_pool->clone_pool_hit = 0;
+	}
+
+	return 1;
+}
+static const struct file_operations skb_pool_fops = {
+	.owner	 = THIS_MODULE,
+	.open    = skb_pool_seq_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release,
+	.write   = skb_pool_reset,
+};
+
+static inline void skb_init_pool_id(void *foo)
+{
+	struct sk_buff *skb = (struct sk_buff *)foo;
+
+	skb->pool_id = -1;
+}
+
+static inline void skb_init_pool_id_clone(void *foo)
+{
+	struct sk_buff *skb, *cskb;
+
+	skb = (struct sk_buff *)foo;
+	cskb = skb + 1;
+
+	skb->pool_id = cskb->pool_id = -1;
+}
+
 void __init skb_init(void)
 {
+	int cpu;
+
 	skbuff_head_cache = kmem_cache_create("skbuff_head_cache",
 					      sizeof(struct sk_buff),
 					      0,
 					      SLAB_HWCACHE_ALIGN|SLAB_PANIC,
-					      NULL);
+					      skb_init_pool_id);
 	skbuff_fclone_cache = kmem_cache_create("skbuff_fclone_cache",
 						(2*sizeof(struct sk_buff)) +
 						sizeof(atomic_t),
 						0,
 						SLAB_HWCACHE_ALIGN|SLAB_PANIC,
+						skb_init_pool_id_clone);
+
+	fastsocket_skbuff_head_cache = kmem_cache_create("fastsocket_skbuff_head_cache", 
+						sizeof(struct sk_buff), 
+						0, 
+						SLAB_HWCACHE_ALIGN | SLAB_PANIC, 
+						NULL);
+	fastsocket_skbuff_fclone_cache = kmem_cache_create("fastsocket_skbuff_fclone_cache", 
+						(2*sizeof(struct sk_buff)) + 
+						sizeof(atomic_t), 
+						0, 
+						SLAB_HWCACHE_ALIGN | SLAB_PANIC, 
 						NULL);
+
+	skb_pools = alloc_percpu(struct skb_pool);
+	//skb_clone_pools = alloc_percpu(struct skb_pool);
+
+	for_each_online_cpu(cpu) {
+		int i;
+		struct skb_pool *skb_pool = per_cpu_ptr(skb_pools, cpu);
+		//struct skb_pool *skb_clone_pool = per_cpu_ptr(skb_clone_pools, cpu);
+		struct sk_buff *skb;
+
+		skb_queue_head_init(&skb_pool->free_list);
+		skb_queue_head_init(&skb_pool->recyc_list);
+		skb_pool->pool_hit = skb_pool->slab_hit = 0;
+
+		skb_queue_head_init(&skb_pool->clone_free_list);
+		skb_queue_head_init(&skb_pool->clone_recyc_list);
+		skb_pool->clone_pool_hit = skb_pool->clone_slab_hit = 0;
+
+		for (i = 0; i < MAX_FASTSOCKET_POOL_SKB_NUM; i++) {
+			//FIXME: GFP_FLAG may take some considieration.
+			//FIXME: Need more carefull release.
+			skb = kmem_cache_alloc_node(fastsocket_skbuff_head_cache, GFP_KERNEL, cpu_to_node(cpu));
+			skb->data_cache = kmalloc_node(MAX_FASTSOCKET_SKB_RAW_SIZE, 
+					GFP_KERNEL, cpu_to_node(cpu));
+			skb->pool_id = cpu;
+			skb_queue_head(&skb_pool->free_list, skb);
+		}
+		for (i = 0; i < MAX_FASTSOCKET_POOL_SKB_NUM; i++) {
+			//FIXME: GFP_FLAG may take some considieration.
+			//FIXME: Need more carefull release.
+			skb = kmem_cache_alloc_node(fastsocket_skbuff_fclone_cache, GFP_KERNEL, cpu_to_node(cpu));
+			skb->data_cache = kmalloc_node(MAX_FASTSOCKET_SKB_RAW_SIZE, 
+					GFP_KERNEL, cpu_to_node(cpu));
+			skb->pool_id = cpu;
+			skb_queue_head(&skb_pool->clone_free_list, skb);
+		}
+	}
+	
+	proc_net_fops_create(&init_net, "skb_pool", S_IRUGO, &skb_pool_fops);
 }
 
 /**
