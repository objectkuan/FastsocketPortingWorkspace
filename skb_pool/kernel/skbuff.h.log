diff --git a/kernel/include/linux/skbuff.h b/kernel/include/linux/skbuff.h
index 17a87cd..4cd2d43 100644
--- a/kernel/include/linux/skbuff.h
+++ b/kernel/include/linux/skbuff.h
@@ -30,6 +30,11 @@
 #include <linux/dmaengine.h>
 #include <linux/hrtimer.h>
 
+#include <linux/hardirq.h>
+
+//#define FPRINTK(msg, args...) printk(KERN_DEBUG "Fastsocket [CPU%d] %s:%d\t" msg, smp_processor_id(), __FUNCTION__, __LINE__, ## args);
+#define FPRINTK(msg, args...)
+
 /* Don't change this without changing skb_csum_unnecessary! */
 #define CHECKSUM_NONE 0
 #define CHECKSUM_UNNECESSARY 1
@@ -225,6 +230,17 @@ struct skb_shared_info {
 	void *		destructor_arg;
 };
 
+struct skb_pool {
+	struct sk_buff_head free_list;
+	struct sk_buff_head recyc_list;
+	struct sk_buff_head clone_free_list;
+	struct sk_buff_head clone_recyc_list;
+	unsigned long pool_hit;
+	unsigned long slab_hit;
+	unsigned long clone_pool_hit;
+	unsigned long clone_slab_hit;
+};
+
 /* We divide dataref into two halves.  The higher 16 bits hold references
  * to the payload part of skb->data.  The lower 16 bits hold references to
  * the entire skb->data.  A clone of a headerless skb holds the length of
@@ -246,6 +262,11 @@ enum {
 	SKB_FCLONE_CLONE,
 };
 
+#define SLAB_SKB	0
+#define SLAB_SKB_CLONE	1
+#define POOL_SKB	2
+#define POOL_SKB_CLONE	3
+
 enum {
 	SKB_GSO_TCPV4 = 1 << 0,
 	SKB_GSO_UDP = 1 << 1,
@@ -427,11 +448,17 @@ struct sk_buff {
 	sk_buff_data_t		tail;
 	sk_buff_data_t		end;
 	unsigned char		*head,
-				*data;
+				*data,
+				*data_cache;
 	unsigned int		truesize;
+	int			pool_id;
 	atomic_t		users;
 };
 
+#define MAX_FASTSOCKET_SKB_RAW_SIZE     ( 2048 )
+#define MAX_FASTSOCKET_SKB_DATA_SIZE    ( 2048 - sizeof(struct skb_shared_info) )
+#define MAX_FASTSOCKET_POOL_SKB_NUM     ( 1024 )
+
 #ifdef __KERNEL__
 /*
  *	Handling routines are only of interest to the kernel
@@ -460,16 +487,50 @@ extern void consume_skb(struct sk_buff *skb);
 extern void	       __kfree_skb(struct sk_buff *skb);
 extern struct sk_buff *__alloc_skb(unsigned int size,
 				   gfp_t priority, int fclone, int node);
+
+extern int enable_skb_pool;
+
 static inline struct sk_buff *alloc_skb(unsigned int size,
 					gfp_t priority)
 {
-	return __alloc_skb(size, priority, 0, -1);
+	struct sk_buff *skb;
+
+	if (enable_skb_pool && likely(in_softirq())) {
+		//printk(KERN_DEBUG "Allocate pool skb in interrupt\n");
+		skb = __alloc_skb(size, priority, POOL_SKB, -1);
+	} else {
+		//printk(KERN_DEBUG "Allocate pool skb NOT in softirq\n");
+		skb = __alloc_skb(size, priority, 0, -1);
+	}
+
+	FPRINTK("Allocate skb 0x%p\n", skb);
+
+	return skb;
 }
 
 static inline struct sk_buff *alloc_skb_fclone(unsigned int size,
 					       gfp_t priority)
 {
-	return __alloc_skb(size, priority, 1, -1);
+	struct sk_buff *skb;
+
+	if (enable_skb_pool && likely(!in_interrupt())) {
+		//printk(KERN_DEBUG "Allocate clone pool skb 0x%p NOT in interrupt\n", skb);
+		local_bh_disable();
+		skb = __alloc_skb(size, priority, POOL_SKB_CLONE, -1);
+		local_bh_enable();
+	} else {
+		skb = __alloc_skb(size, priority, 1, -1);
+	}
+
+	FPRINTK("Allocate clone skb 0x%p\n", skb);
+
+	return skb;
+}
+
+static inline struct sk_buff *alloc_pool_skb_fclone(unsigned int size,
+					       gfp_t priority)
+{
+	return __alloc_skb(size, priority, POOL_SKB_CLONE, -1);
 }
 
 extern int skb_recycle_check(struct sk_buff *skb, int skb_size);
@@ -1474,10 +1535,20 @@ static inline void __skb_queue_purge(struct sk_buff_head *list)
  *
  *	%NULL is returned if there is no free memory.
  */
+
 static inline struct sk_buff *__dev_alloc_skb(unsigned int length,
 					      gfp_t gfp_mask)
 {
-	struct sk_buff *skb = alloc_skb(length + NET_SKB_PAD, gfp_mask);
+	struct sk_buff *skb;
+
+	if (enable_skb_pool) {
+		skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, POOL_SKB, -1);
+		FPRINTK("Allocate pool skb 0x%p\n", skb);
+	} else {
+		skb = alloc_skb(length + NET_SKB_PAD, gfp_mask);
+		FPRINTK("Allocate regular skb 0x%p\n", skb);
+	}
+
 	if (likely(skb))
 		skb_reserve(skb, NET_SKB_PAD);
 	return skb;
